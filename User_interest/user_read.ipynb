{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "url = \"https://medium.com/@srjoglekar246/first-time-with-kaggle-a-convnet-to-classify-toxic-comments-with-keras-ef84b6d18328\"\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0'}\n",
    "req = urllib.request.Request(url=url, headers=headers)\n",
    "html = urllib.request.urlopen(req).read()\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s.extract() for s in soup('script')]\n",
    "[s.extract() for s in soup('style')]\n",
    "str_body = soup.get_text()\n",
    "str_body = re.sub(r'\\d|\\$|\\¥|\\[|\\]|\\)|\\(|\\\\', '', str_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converging at iteration 25....\n",
      "[['F'], ['K'], ['C'], ['N'], ['K'], ['H'], ['B'], ['S'], ['G'], ['H'], ['M'], ['S'], ['J'], ['B'], ['U'], ['F'], ['F'], ['E'], ['G'], ['M'], ['L'], ['J'], ['J'], ['F'], ['K'], ['C'], ['N'], ['K'], ['W'], ['K'], ['J'], ['T'], ['C'], ['C'], ['C'], ['M'], ['T'], ['C'], ['N'], ['C'], ['N'], ['K'], ['M', 'L', 'P'], ['T'], ['T'], ['C', 'S', 'V'], ['E'], ['H'], ['C', 'S', 'V'], ['C', 'S', 'V'], ['H'], ['T'], ['B'], ['N', 'L', 'T', 'K', 'T'], ['K'], ['T'], ['S'], ['j'], ['T'], ['M'], ['M'], ['X'], ['S', 'E', 'Q', 'U', 'E', 'N', 'C', 'E'], ['L', 'E', 'N', 'G', 'T', 'H', 'V'], ['L'], ['T'], ['O', 'N'], ['S', 'P', 'L'], ['T'], ['N'], ['N'], ['T'], ['M'], ['X'], ['S', 'E', 'Q', 'U', 'E', 'N', 'C', 'E'], ['L', 'E', 'N', 'G', 'T', 'H'], ['V'], ['L'], ['T'], ['O', 'N'], ['S', 'P', 'L'], ['T'], ['W'], ['E'], ['F'], ['G'], ['T'], ['O'], ['W'], ['V'], ['F'], ['W'], ['V'], ['G'], ['F'], ['F'], ['O', 'O', 'V'], ['G'], ['P'], ['G'], ['O'], ['K'], ['V'], ['K'], ['V'], ['j'], ['W', 'O', 'R'], ['V', 'E', 'C'], ['F', 'O', 'L'], ['E', 'R'], ['F'], ['K'], ['E'], ['M'], ['X'], ['S', 'E', 'Q', 'U', 'E', 'N', 'C', 'E'], ['L', 'E', 'N', 'G', 'T', 'H'], ['T'], ['N'], ['T'], ['W'], ['j'], ['T'], ['T'], ['L'], ['T'], ['C', 'N', 'N'], ['C', 'N', 'N'], ['O'], ['C', 'N', 'N'], ['K'], ['B'], ['F'], ['C', 'N', 'N'], ['N', 'L', 'P'], ['B'], ['j'], ['W'], ['C'], ['M'], ['P'], ['M'], ['N'], ['T'], ['R', 'G', 'E', 'T'], ['C', 'L'], ['S', 'S', 'E', 'S'], ['M'], ['X'], ['S', 'E', 'Q', 'U', 'E', 'N', 'C', 'E'], ['L', 'E', 'N', 'G', 'T', 'H'], ['C'], ['M'], ['P'], ['C'], ['M'], ['P'], ['F'], ['N'], ['T'], ['R', 'G', 'E', 'T'], ['C', 'L'], ['S', 'S', 'E', 'S'], ['M'], ['S'], ['S'], ['j'], ['T'], ['T'], ['K'], ['F'], ['R'], ['T'], ['j'], ['K'], ['S'], ['K'], ['F'], ['Y'], ['T'], ['K'], ['K'], ['C'], ['T'], ['S'], ['O'], ['j'], ['S'], ['T'], ['T'], ['F'], ['N'], ['N'], ['F'], ['N'], ['M'], ['T'], ['E'], ['M'], ['K'], ['X', 'G', 'B'], ['L', 'S', 'T', 'M', 'C', 'N', 'N', 'K'], ['W'], ['K'], ['T'], ['j'], ['K'], ['F'], ['M'], ['j'], ['T'], ['K'], ['Y'], ['L'], ['M'], ['C'], ['T'], ['P'], ['T'], ['M', 'L'], ['C'], ['C'], ['B'], ['j'], ['M'], ['L'], ['K'], ['C'], ['N'], ['K'], ['P'], ['L'], ['G'], ['S'], ['J'], ['F'], ['j'], ['B'], ['U'], ['F'], ['F'], ['S'], ['J'], ['E'], ['G'], ['M'], ['L'], ['J']]\n",
      "Keywords:\n",
      "\n",
      "0L S T M C N N K, \n",
      "1S E Q U E N C E, \n",
      "2L E N G T H V, \n",
      "3N L T K T, \n",
      "4L E N G T H, \n",
      "5S S E S, \n",
      "6R G E T, \n",
      "7C N N, \n",
      "8V E C, \n",
      "9C S V, \n",
      "10N L P, \n",
      "11S P L, \n",
      "12M L P, \n",
      "13F O L, \n",
      "14C L, \n",
      "15M L, \n",
      "16O N, \n",
      "17E R, \n",
      "18O O V, \n",
      "19X G B, \n",
      "20W O R, \n",
      "21j, \n",
      "22J, \n",
      "23Y, \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 24 is out of bounds for axis 0 with size 24",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-dcd45225cacc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeywords_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msorted_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\", \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 24 is out of bounds for axis 0 with size 24"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "movie_content4 = []\n",
    "for i in str_body:\n",
    "    i = re.sub(r'[0-9]|/d+', '',i)\n",
    "    i = re.sub('\\s+', '', str(i))\n",
    "    if i != '' and len(i) <= 15 and re.search(r'[A-za-z]', i):\n",
    "        movie_content4.append(i)\n",
    "#词性标注\n",
    "text = movie_content4\n",
    "POS_tag = nltk.pos_tag(text)\n",
    "#print(POS_tag)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "adjective_tags = ['JJ','JJR','JJS']\n",
    "#根据词性进行词根还原\n",
    "lemmatized_text = []\n",
    "for word in POS_tag:\n",
    "    if word[1] in adjective_tags:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "    else:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "#print (lemmatized_text)\n",
    "#标注好的还原过的单词\n",
    "POS_tag = nltk.pos_tag(lemmatized_text)\n",
    "#print(POS_tag)\n",
    "\n",
    "stopwords2 = []\n",
    "wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "for word in POS_tag:\n",
    "    if word[1] not in wanted_POS:\n",
    "        stopwords2.append(word[0])\n",
    "\n",
    "punctuations = list(str(string.punctuation))\n",
    "stopwords2 = stopwords2 + punctuations\n",
    "\n",
    "stopwords_plus = []\n",
    "stopwords_plus = stopwords2 + stop\n",
    "stopwords_plus = set(stopwords_plus)\n",
    "\n",
    "processed_text = []\n",
    "for word in lemmatized_text:\n",
    "    if word not in stopwords_plus:\n",
    "        processed_text.append(word)\n",
    "    \n",
    "vocabulary = list(set(processed_text))\n",
    "import numpy as np\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "\n",
    "weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "score = np.zeros((vocab_len),dtype=np.float32)\n",
    "window_size = 3\n",
    "covered_coocurrences = []\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    score[i]=1\n",
    "    for j in range(0,vocab_len):\n",
    "        if j==i:\n",
    "            weighted_edge[i][j]=0\n",
    "        else:\n",
    "            for window_start in range(0,(len(processed_text)-window_size)):\n",
    "                window_end = window_start+window_size\n",
    "                window = processed_text[window_start:window_end]\n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    \n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    # index_of_x is the absolute position of the xth term in the window \n",
    "                    # (counting from 0) \n",
    "                    # in the processed_text\n",
    "                      \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                        covered_coocurrences.append([index_of_i,index_of_j])\n",
    "\n",
    "inout = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    for j in range(0,vocab_len):\n",
    "        inout[i]+=weighted_edge[i][j]\n",
    "        \n",
    "MAX_ITERATIONS = 50\n",
    "d=0.85\n",
    "threshold = 0.0001 #convergence threshold\n",
    "\n",
    "for iter in range(0,MAX_ITERATIONS):\n",
    "    prev_score = np.copy(score)\n",
    "    \n",
    "    for i in range(0,vocab_len):\n",
    "        \n",
    "        summation = 0\n",
    "        for j in range(0,vocab_len):\n",
    "            if weighted_edge[i][j] != 0:\n",
    "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "                \n",
    "        score[i] = (1-d) + d*(summation)\n",
    "    \n",
    "    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
    "        print (\"Converging at iteration \"+str(iter)+\"....\")\n",
    "        break\n",
    "        \n",
    "phrases = []\n",
    "\n",
    "phrase = \" \"\n",
    "for word in lemmatized_text:\n",
    "    \n",
    "    if word in stopwords2:\n",
    "        if phrase!= \" \":\n",
    "            phrases.append(str(phrase).strip().split())\n",
    "        phrase = \" \"\n",
    "    elif word not in stopwords2:\n",
    "        phrase+=str(word)\n",
    "        phrase+=\" \"\n",
    "\n",
    "#print (\"Partitioned Phrases (Candidate Keyphrases): \\n\")\n",
    "print (phrases)\n",
    "\n",
    "unique_phrases = []\n",
    "\n",
    "for phrase in phrases:\n",
    "    if phrase not in unique_phrases:\n",
    "        unique_phrases.append(phrase)\n",
    "        \n",
    "for word in vocabulary:\n",
    "    #print word\n",
    "    for phrase in unique_phrases:\n",
    "        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
    "            unique_phrases.remove([word])\n",
    "\n",
    "phrase_scores = []\n",
    "keywords = []\n",
    "for phrase in unique_phrases:\n",
    "    phrase_score=0\n",
    "    keyword = ''\n",
    "    for word in phrase:\n",
    "        keyword += str(word)\n",
    "        keyword += \" \"\n",
    "        phrase_score+=score[vocabulary.index(word)]\n",
    "    phrase_scores.append(phrase_score)\n",
    "    keywords.append(keyword.strip())\n",
    "\n",
    "sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "\n",
    "keywords_num = 30\n",
    "\n",
    "print (\"Keywords:\\n\")\n",
    "\n",
    "for i in range(0,keywords_num):\n",
    "    print (str(i) + str(keywords[sorted_index[i]])+\", \",)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
