{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import string\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the word list from url\n",
    "def UrlToWords(url):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0'}\n",
    "    req = urllib.request.Request(url=url, headers=headers)\n",
    "    try:\n",
    "        html = urllib.request.urlopen(req).read()\n",
    "    except:\n",
    "        return \n",
    "    else:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        st = LancasterStemmer()\n",
    "        [s.extract() for s in soup('script')]\n",
    "        [s.extract() for s in soup('style')]\n",
    "        t_string = soup.get_text()\n",
    "        tokens = nltk.word_tokenize(t_string)\n",
    "        text = [w.lower() for w in tokens]\n",
    "\n",
    "        all_p = punctuation + '--' + '|' +'-' + '~' + '``' + '\"' + '=' + '_' + '\\\"'+ '/'\n",
    "        words = []\n",
    "        for i in text:\n",
    "            i = re.sub('\\s+', '', str(i))\n",
    "            i = re.sub('\\\\\\\\|\\^|\\_', '' , i)\n",
    "            i = re.sub('[^A-Za-z]', '', i)\n",
    "            if i != '' and len(i) <= 15  and i not in all_p:\n",
    "                words.append(i)\n",
    "\n",
    "        POS_tag = nltk.pos_tag(words)\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        adjective_tags = ['JJ','JJR','JJS']\n",
    "        #根据词性进行词根还原\n",
    "        lemmatized_text = []\n",
    "        for word in POS_tag:\n",
    "            if word[1] in adjective_tags:\n",
    "                lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "            else:\n",
    "                lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "        words = lemmatized_text\n",
    "        POS_tag = nltk.pos_tag(words)\n",
    "\n",
    "        stopwords2 = []\n",
    "        wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "        for word in POS_tag:\n",
    "            if word[1] not in wanted_POS:\n",
    "                stopwords2.append(word[0])\n",
    "\n",
    "        punctuations = list(str(string.punctuation))\n",
    "        stopwords2 = stopwords2 + punctuations\n",
    "        stop = stopwords.words('english')\n",
    "        stopwords_plus = stopwords2 + stop\n",
    "        stopwords_plus = set(stopwords_plus)\n",
    "\n",
    "        processed_text = []\n",
    "        for word in words:\n",
    "            if word not in stopwords_plus:\n",
    "                processed_text.append(word)\n",
    "\n",
    "        words = processed_text\n",
    "        words = [w for w in words if len(w)>2]\n",
    "        #words = [w for w in words if w not in all_p]\n",
    "        fre = collections.Counter(words)\n",
    "        #fre = fre.most_common()\n",
    "        fre = dict(fre)\n",
    "        t1 = []\n",
    "        t2 = []\n",
    "        for k,v in fre.items():\n",
    "            if v==1:\n",
    "                t1.append(k)\n",
    "        #frequences = dict(zip(t1,t2))\n",
    "        words = [w for w in words if w not in t1]\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/\n",
      "http://www.cnblogs.com/pinard/p/6244265.html\n",
      "http://www.cnblogs.com/pinard/p/6239403.html\n",
      "http://blog.csdn.net/zengxiantao1994/article/details/72787849\n",
      "http://blog.csdn.net/gao1440156051/article/details/44162269\n",
      "https://my.oschina.net/findbill/blog/535044\n",
      "http://www.ifis.uni-luebeck.de/~moeller/Lectures/WS-16-17/Web-Mining-Agents/PCA-SVD.pdf\n",
      "http://bigdata-madesimple.com/decoding-dimensionality-reduction-pca-and-svd/\n",
      "https://jockchou.gitbooks.io/getting-started-with-mongodb/content/book/import.html\n",
      "http://xxuan.me/2017-02-17-mongodb.html\n",
      "http://www.open-open.com/lib/view/open1414223469247.html\n",
      "https://www.cnblogs.com/rongyux/p/5602037.html\n",
      "https://www.datacamp.com/courses/intro-to-python-for-data-science\n",
      "http://blog.csdn.net/zm714981790/article/details/51304506\n",
      "http://blog.csdn.net/sinat_26917383/article/details/53260117\n",
      "http://blog.csdn.net/lily960427/article/details/78996500\n",
      "https://github.com/jerry81333/SpamSMSFiltering\n",
      "http://brandonrose.org/clustering_mobile#Hierarchical-document-clustering\n",
      "https://bl.ocks.org/mbostock/2368837\n",
      "http://blog.csdn.net/gdp12315_gu/article/details/51721988\n",
      "http://blog.csdn.net/tianxuzhang/article/details/14103477\n",
      "https://github.com/brandomr/document_cluster\n",
      "http://bl.ocks.org/juan-cb/faf62e91e3c70a99a306\n",
      "http://www.xiazaiba.com/jiaocheng/15303.html\n",
      "https://github.com/shakibbinhamid/sjdb-comp3211/blob/master/src/sjdb/Optimiser.java\n",
      "https://jizhi.im/blog/post/comment-classification\n",
      "https://www.oschina.net/translate/big-picture-machine-learning\n",
      "https://www.jianshu.com/p/9778bdc5982b\n",
      "https://blog.csdn.net/u010223750/article/details/53334313\n",
      "https://blog.csdn.net/lovebyz/article/details/77712003\n",
      "https://blog.csdn.net/zzulp/article/details/76146947\n",
      "https://medium.com/@srjoglekar246/first-time-with-kaggle-a-convnet-to-classify-toxic-comments-with-keras-ef84b6d18328\n",
      "https://github.com/surupendu/IRProject/blob/master/Toxic%20Comment%20Detection%20(Multinomial%20Naive%20Bayes).ipynb\n",
      "https://github.com/rohit-136/toxic_classification/blob/master/Final_Report%20.ipynb\n",
      "https://blog.csdn.net/szuodao/article/details/51743501\n",
      "https://docs.djangoproject.com/zh-hans/2.0/intro/tutorial01/\n",
      "https://chartjs.bootcss.com/docs/\n",
      "https://blog.csdn.net/han_xiaoyang/article/details/50629608\n",
      "https://blog.csdn.net/pipisorry/article/details/52574156\n",
      "https://blog.csdn.net/heyongluoyao8/article/details/49408319\n",
      "https://www.cnblogs.com/pinard/p/6074222.html\n",
      "https://speechlab.sjtu.edu.cn/pages/sz128/homepage/year/10/14/micro-average-and-macro-average/\n",
      "http://www.gooseeker.com/cn/node/Fuller/2010051401\n",
      "https://blog.csdn.net/u011138533/article/details/72629728\n",
      "https://blog.csdn.net/pluschang/article/details/78425523\n",
      "http://platform.fatsecret.com/api/\n",
      "http://sussed.soton.ac.uk/\n",
      "https://www.southampton.ac.uk/isolutions/students/southampton-virtual-environment.page\n",
      "https://outlook.office.com/owa/?realm=soton.ac.uk\n",
      "https://secure.ecs.soton.ac.uk/student/\n",
      "https://sspai.com/post/40457\n",
      "https://sspai.com/post/27577\n",
      "http://qunar.zhiye.com/schooljobshow?jobId=620160070\n",
      "https://www.zhiyeapp.com/\n",
      "http://p.bwgrt.com/jianli\n",
      "https://sspai.com/post/38325\n",
      "https://platform.fatsecret.com/api/Default.aspx?screen=jsapih\n",
      "https://wuxianglong.gitbooks.io/django-tutorial/content/chapter02/1-model-intro-example.html\n",
      "https://www.kancloud.cn/wangkang_reg/django_basic/180745\n",
      "https://www.mohu.org/info/symbols/symbols.htm\n",
      "https://chinadigitaltimes.net/chinese/2016/08/%e4%b8%ad%e5%9b%bd%e6%95%b0%e5%ad%97%e6%97%b6%e4%bb%a3china-digital-times-cdt%e5%85%8d%e7%bf%bb%e5%a2%99%e6%b5%8f%e8%a7%88%e5%99%a8%e6%89%a9%e5%b1%95/\n",
      "https://github.com/ibm-watson-iot/predictive-analytics-samples\n",
      "https://www.cnblogs.com/Free-Thinker/p/7298781.html\n",
      "https://aws.amazon.com/cn/about-aws/events/webinar/iot-solutions-based-on-aws-iot-platform06062017/\n",
      "https://www.smartsheet.com/\n",
      "https://www.cnblogs.com/liuxianan/p/chrome-plugin-develop.html\n",
      "https://blog.csdn.net/wang124454731/article/details/78911220\n",
      "http://dataunion.org/29741.html\n",
      "https://www.jiqizhixin.com/articles/2017-02-16\n",
      "https://pdfs.semanticscholar.org/3673/bccde93025e05431a2bcac4e8ff18c9c273a.pdf\n",
      "https://github.com/kent666a/kent-resources/blob/master/bookmarks.md\n",
      "https://developer.mozilla.org/zh-CN/docs/Learn/Server-side/Django/Deployment\n",
      "https://github.com/kent666a/py-chrome-bookmarks-markdown\n",
      "http://funhacks.net/explore-python/Standard-Modules/argparse.html\n",
      "https://github.com/TheInsomniac/browser-bookmarks/blob/master/firefox%20bookmarks.py\n",
      "http://code.activestate.com/recipes/579077-bookmarks-browser-for-firefox/\n",
      "http://www.geerniya.cn/blog/18/\n",
      "https://meetme.so/thanassis\n",
      "https://blog.csdn.net/oopsoom/article/details/25389285\n",
      "https://cn.linkedin.com/pulse/netflix%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%9C%80%E6%96%B0%E8%A7%A3%E8%AF%BB%E7%AE%97%E6%B3%95%E5%95%86%E4%B8%9A%E4%BB%B7%E5%80%BC%E4%B8%8E%E5%88%9B%E6%96%B0-%E6%96%87%E6%A0%8B-%E8%B0%B7\n",
      "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3\n",
      "file:///C:/Users/lujun/Desktop/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0.pdf\n",
      "https://datartisan.gitbooks.io/begining-text-mining-with-python/content/%E7%AC%AC5%E7%AB%A0%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8C%96%E5%A4%84%E7%90%86/5.1%20%E6%96%87%E6%A1%A3-%E8%AF%8D%E9%A1%B9%E7%9F%A9%E9%98%B5.html\n",
      "http://beautifulsoup.readthedocs.io/zh_CN/latest/#id18\n",
      "https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html\n",
      "https://www.cnblogs.com/qniguoym/p/7772561.html\n",
      "https://blog.csdn.net/diye2008/article/details/53105652?locationNum=11&fps=1\n",
      "http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
      "http://www.52nlp.cn/%e5%a6%82%e4%bd%95%e8%ae%a1%e7%ae%97%e4%b8%a4%e4%b8%aa%e6%96%87%e6%a1%a3%e7%9a%84%e7%9b%b8%e4%bc%bc%e5%ba%a6%e4%b8%80\n",
      "https://blog.csdn.net/AndrewLee_/article/details/55095538\n",
      "https://blog.csdn.net/xiewenbo/article/details/46671587\n",
      "https://blog.csdn.net/jk981811667/article/details/78958737\n",
      "https://blog.csdn.net/free356/article/details/79445895\n",
      "https://github.com/JRC1995/TextRank-Keyword-Extraction/blob/master/TextRank.ipynb\n",
      "https://blog.csdn.net/u013074302/article/details/76422551\n",
      "https://blog.csdn.net/John_xyz/article/details/79424284\n",
      "https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n",
      "https://blog.csdn.net/u014595019/article/details/52218249\n",
      "http://www.pythoner.com/205.html\n"
     ]
    }
   ],
   "source": [
    "#url = 'http://www.skysports.com/football/news/11095/11464517/thibaut-courtois-eden-hazard-toby-alderweireld-premier-leagues-belgian-stars-on-the-move'\n",
    "f = open('url_list.txt', 'r')\n",
    "user_url = f.read().splitlines()\n",
    "f.close()\n",
    "user_data = []\n",
    "for url in user_url:\n",
    "    print(url)\n",
    "    words = UrlToWords(url)\n",
    "    if words:\n",
    "        words = list(set(words))\n",
    "        user_data.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4524"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = open('user_data.txt', 'w', encoding='utf-8')\n",
    "for x in user_data:\n",
    "    if x:\n",
    "        w.write(i)\n",
    "        w.write('\\n')\n",
    "    else:\n",
    "        continue\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words frequences greater than p \n",
    "import collections\n",
    "def get_top_fre(words, p):\n",
    "    fre = collections.Counter(words)\n",
    "    fre = fre.most_common()\n",
    "    fre = dict(fre)\n",
    "    t1 = []\n",
    "    t2 = []\n",
    "    for k,v in fre.items():\n",
    "        if v>p:\n",
    "            t1.append(k)\n",
    "            t2.append(v)\n",
    "    #frequences = dict(zip(t1,t2))\n",
    "    return t1\n",
    "\n",
    "def get_count(words):\n",
    "    fre = collections.Counter(words)\n",
    "    return fre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "def get_wordcloud(words):\n",
    "    wordcloud = WordCloud(background_color=\"white\", width=1000, height=860, margin=2).generate(str(words))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    wordcloud.to_file('text.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-23 21:34:34,595 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-07-23 21:34:34,598 : INFO : built Dictionary(815 unique tokens: ['analys', 'big', 'compon', 'dat', 'diagon']...) from 11 documents (total 939 corpus positions)\n",
      "2018-07-23 21:34:34,600 : INFO : collecting document frequencies\n",
      "2018-07-23 21:34:34,602 : INFO : PROGRESS: processing document #0\n",
      "2018-07-23 21:34:34,604 : INFO : calculating IDF weights for 11 documents and 814 features (939 matrix non-zeros)\n",
      "2018-07-23 21:34:34,607 : INFO : using serial LSI version on this node\n",
      "2018-07-23 21:34:34,607 : INFO : updating model with new documents\n",
      "2018-07-23 21:34:34,612 : INFO : preparing a new chunk of documents\n",
      "2018-07-23 21:34:34,614 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-07-23 21:34:34,615 : INFO : 1st phase: constructing (815, 110) action matrix\n",
      "2018-07-23 21:34:34,623 : INFO : orthonormalizing (815, 110) action matrix\n",
      "2018-07-23 21:34:34,668 : INFO : 2nd phase: running dense svd on (110, 11) matrix\n",
      "2018-07-23 21:34:34,676 : INFO : computing the final decomposition\n",
      "2018-07-23 21:34:34,677 : INFO : keeping 10 factors (discarding 8.041% of energy spectrum)\n",
      "2018-07-23 21:34:34,679 : INFO : processed documents up to #11\n",
      "2018-07-23 21:34:34,682 : INFO : topic #0(1.098): 0.102*\"footbal\" + 0.102*\"team\" + 0.099*\"learn\" + 0.096*\"world\" + 0.094*\"'s\" + 0.091*\"word\" + 0.091*\"model\" + 0.090*\"hour\" + 0.090*\"vaccin\" + 0.090*\"ago\"\n",
      "2018-07-23 21:34:34,684 : INFO : topic #1(1.047): 0.184*\"footbal\" + 0.184*\"team\" + 0.159*\"sky\" + 0.159*\"tv\" + 0.159*\"town\" + 0.159*\"salah\" + 0.159*\"fc\" + 0.159*\"liverpool\" + 0.159*\"kari\" + 0.159*\"real\"\n",
      "2018-07-23 21:34:34,686 : INFO : topic #2(1.025): -0.422*\"fatsecret\" + -0.422*\"platform\" + -0.375*\"ap\" + -0.106*\"martijn\" + -0.106*\"json\" + -0.106*\"jsondecodeer\" + -0.106*\"overflow\" + -0.106*\"pars\" + -0.106*\"ask\" + -0.106*\"exceiv\"\n",
      "2018-07-23 21:34:34,688 : INFO : topic #3(1.011): 0.252*\"ago\" + 0.252*\"hour\" + 0.252*\"vaccin\" + 0.193*\"day\" + 0.184*\"chines\" + 0.184*\"trad\" + 0.184*\"chin\" + 0.119*\"world\" + 0.097*\"nhs\" + 0.097*\"night\"\n",
      "2018-07-23 21:34:34,690 : INFO : topic #4(1.003): 0.167*\"analys\" + 0.167*\"eig\" + 0.167*\"matrix\" + 0.167*\"big\" + 0.167*\"im\" + 0.167*\"compon\" + 0.167*\"princip\" + 0.167*\"way\" + 0.167*\"svd\" + 0.167*\"diagon\"\n",
      "2018-07-23 21:34:34,692 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2018-07-23 21:34:34,694 : INFO : creating matrix with 11 documents and 10 features\n",
      "D:\\Anaconda\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = texts[5]\n",
    "bow = dictionary.doc2bow(test)\n",
    "ml_lsi = lsi[bow]\n",
    "sims = index[ml_lsi]\n",
    "sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.102*\"footbal\" + 0.102*\"team\" + 0.099*\"learn\" + 0.096*\"world\" + 0.094*\"\\'s\" + 0.091*\"word\" + 0.091*\"model\" + 0.090*\"hour\" + 0.090*\"vaccin\" + 0.090*\"ago\"'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topic(topicno=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
